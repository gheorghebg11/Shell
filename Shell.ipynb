{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shell.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "IhDqpEz1xG3u",
        "JNN8go-BB9RJ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gheorghebg11/Shell/blob/master/Shell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ksXJw2tBpWrW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load the files into Colab"
      ]
    },
    {
      "metadata": {
        "id": "FKZMLM-S16Nd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first cells are for loading the big data files from google drive (as colab only keeps the files for 12 hours), and the .csv from my local drive or dropbox. We first authenticate."
      ]
    },
    {
      "metadata": {
        "id": "WzIkU8LMyozt",
        "colab_type": "code",
        "outputId": "1db7b3ea-4f66-497a-ae35-3337aa91043b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive\n",
        "import os, zipfile\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyDrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 26.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.5)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dEMiyKPeq9Hw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now download the files and un unzip the archives. Right now there are 4 folders for 4 different gas station brands."
      ]
    },
    {
      "metadata": {
        "id": "dWbpFmwVqgxv",
        "colab_type": "code",
        "outputId": "fbd2888a-856f-46a0-a948-ffa5bd5a8b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -r /content/data\n",
        "!mkdir /content/data\n",
        "print('\\nDownloading the data')\n",
        "download = drive.CreateFile({'id': '1e5u2uDQ5mupV64KJbmWVG-JhlGKz9uUD'})\n",
        "download.GetContentFile('/content/data_shell.tar.gz')\n",
        "!tar -xzf /content/data_shell.tar.gz -C /content/data\n",
        "print('\\nData successfully downloaded in /content/data')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/data': No such file or directory\n",
            "\n",
            "Downloading the data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IhDqpEz1xG3u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Visualization and Basic Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "dqBNmbTp12LL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First load some packages for Data Visualization."
      ]
    },
    {
      "metadata": {
        "id": "1GgouP0vzvop",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1989)\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import display\n",
        "pd.options.display.max_columns = 20\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lCb-h9hm0Mgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Explore the data a little bit. We look at the available categories (each in one separate folder) and create a dictionary for one-hot encoding."
      ]
    },
    {
      "metadata": {
        "id": "i45IxsCd0MWJ",
        "colab_type": "code",
        "outputId": "da18478a-c27c-45cd-b152-5077d5d0c365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dir_data = '/content/data/'\n",
        "labels = list(os.listdir(dir_data))\n",
        "labels_to_idx = {label: i for i, label in enumerate(labels)}\n",
        "print(labels_to_idx)\n",
        "\n",
        "#train = pd.read_csv(os.path.join(dir_data, 'train_curated.csv'))\n",
        "#test = pd.read_csv(os.path.join(dir_data, 'sample_submission.csv'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'chevron': 0, 'phillips': 1, 'conoco': 2, 'shell': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ou05IqYl_W5o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now loop through all files and do a few tasks:\n",
        "- check for corrupted files (ending in .part)\n",
        "- check for all available extensions, in order to exlcude non-images\n",
        "- rename each file in the formal 'label_oldfilename' and move them all in one single folder."
      ]
    },
    {
      "metadata": {
        "id": "pdD0NsgB-_9u",
        "colab_type": "code",
        "outputId": "2d4c7987-fe44-429c-d578-fe6c184920a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "extensions = set()\n",
        "\n",
        "for label in labels:\n",
        "    dir_label = os.path.join(dir_data, label)\n",
        "    filenames = [name for name in os.listdir(dir_label) if os.path.isfile(os.path.join(dir_label, name))]\n",
        "    \n",
        "    for filename in filenames:\n",
        "        new_filename = label + '_' + filename\n",
        "        os.rename(os.path.join(dir_label, filename), os.path.join(dir_data,new_filename))\n",
        "        \n",
        "        # check for the extension\n",
        "        extension = filename.split('.')[-1]\n",
        "        if extension not in extensions:\n",
        "            extensions.add(extension)\n",
        "    \n",
        "    # erase the empty folder\n",
        "    os.rmdir(os.path.join(dir_data, label))\n",
        "    #shutil.rmtree(os.path.join(dir_data, label))\n",
        "    \n",
        "print(f'We found the extensions {extensions}')\n",
        "\n",
        "# create a list with all filenames MAYBE PUT IN PANDAS ?\n",
        "filenames = [name for name in os.listdir(dir_data) if os.path.isfile(os.path.join(dir_data, name))]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We found the extensions {'jpeg', 'jpg', 'png', 'part'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-HrOXXZUUES1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now erase the files ending in .part as they are corrupted images that failed during data mining."
      ]
    },
    {
      "metadata": {
        "id": "aOc-LEDqRDMw",
        "colab_type": "code",
        "outputId": "3eaa5760-fccb-4b70-dca1-7b7b0e735d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "corrupted_files = 0\n",
        "for filename in filenames:\n",
        "    if filename.endswith('.part'):\n",
        "        corrupted_files = corrupted_files + 1\n",
        "        os.remove(os.path.join(dir_data, filename))\n",
        "        filenames.remove(filename)\n",
        "print(f'We removed {corrupted_files} corrupted files out of {len(filenames) + corrupted_files} total files.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We removed 7 corrupted files out of 776 total files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I-pA5vZHVEBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now create a Pandas df with the data to explore it a little bit. \n",
        "#TODO"
      ]
    },
    {
      "metadata": {
        "id": "Pfm79SWAVEhW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXozKqnwAK4w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Look at the shape and number of examples and labels. "
      ]
    },
    {
      "metadata": {
        "id": "-cLQ9amV2DPD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(f'Train set has {train.shape[0]} examples and {len(set(train.labels))} different labels')\n",
        "train.sample(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nqZCmtik2N5D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(f'Test set has {test.shape[0]} examples and {len(test.columns[1:])} different labels')\n",
        "test.sample(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kHNxz_HB4szP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will for now exclude the sounds which have multiple labels."
      ]
    },
    {
      "metadata": {
        "id": "WGrqELSj2N9U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = train[train['labels'].isin(test.columns[1:])]\n",
        "print(f'Removing multilabel examples. The Train set has now {train.shape[0]} examples and {len(set(train.labels))} different labels')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8X5o1jcF48s3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's visualize how many samples there are per label."
      ]
    },
    {
      "metadata": {
        "id": "mQxcK7Ja49du",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "category_group = train.groupby(['labels']).count()\n",
        "category_group.columns = ['counts']\n",
        "print(f'The number of training clips per label range from {category_group.counts.min()} to {category_group.counts.max()}')\n",
        "plot = category_group.sort_values(ascending=True, by='counts').plot(kind='barh', title='nbr training audio clips per label', figsize=(20,12))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Cs9GgL886Z6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now visualize the distribution of their length:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GPdoJyuj9nHU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We first only look at the top 25 categories"
      ]
    },
    {
      "metadata": {
        "id": "EQZi4Kii9bcL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train['nframes'] = train['fname'].apply(lambda fname : wave.open(os.path.join(dir_data, 'curated', fname)).getnframes())\n",
        "test['nframes'] = test['fname'].apply(lambda fname : wave.open(os.path.join(dir_data, 'test', fname)).getnframes())\n",
        "# plot the distribution of top 25 catetegories of the training set, since 74 is a little bit too much\n",
        "idx_25_top = category_group.sort_values(ascending=True, by='counts').index[-25:]\n",
        "_, ax = plt.subplots(figsize=(25,10))\n",
        "sns.violinplot(data = train[train.labels.isin(idx_25_top)], x='labels', y='nframes')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C54lYlg1AETw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now compare the training and the test set."
      ]
    },
    {
      "metadata": {
        "id": "jhxUsewn9bh6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2,1, figsize=(20,8))\n",
        "train.nframes.plot(kind='hist', bins=100, rwidth=0.5, ax = ax[0])\n",
        "test.nframes.plot(kind='hist', bins=100, rwidth=0.5, ax = ax[1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WWEXiQBv9Zp-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's pick a random audio clip and look at its attributes."
      ]
    },
    {
      "metadata": {
        "id": "f69j6eQH49f5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rand_ex = train.sample(1)\n",
        "fname = rand_ex['fname'].values[0]\n",
        "path_audiofile = os.path.join(dir_data, 'curated', fname)\n",
        "wav = wave.open(path_audiofile)\n",
        "print(f'Filename is {fname}')\n",
        "print(f'Sampling frame rate {wav.getframerate()}')\n",
        "print(f'Total frames {wav.getnframes()}')\n",
        "print(f'Duration {wav.getnframes() / wav.getframerate()} sec')\n",
        "print(f'Label is {rand_ex.labels.values[0]} \\n' )\n",
        "import IPython\n",
        "IPython.display.Audio(path_audiofile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "amS6TzdR9Hbj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally we now save this cleaned dataframe."
      ]
    },
    {
      "metadata": {
        "id": "Zz2dWcyJ9F_j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.to_csv(os.path.join(dir_data, 'train_curated_clean.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JNN8go-BB9RJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Construct The Model"
      ]
    },
    {
      "metadata": {
        "id": "eRb_Jb_2E9S7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First (Re)Load some Packages"
      ]
    },
    {
      "metadata": {
        "id": "tgzcJyleELS5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1989)\n",
        "\n",
        "import os, shutil, cv2\n",
        "import pandas as pd\n",
        "\n",
        "from albumentations import (Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
        "    RandomBrightness, RandomContrast, RandomGamma, ToFloat, ShiftScaleRotate)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras import losses, models, optimizers\n",
        "from keras.activations import relu, softmax\n",
        "from keras.callbacks import (EarlyStopping, LearningRateScheduler, ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
        "from keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D, GlobalMaxPool1D, Input, MaxPool1D, concatenate) # for the 1D conv models\n",
        "from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten, GlobalMaxPool2D, MaxPool2D, Activation, concatenate) # for the 2D models with MFCC\n",
        "from keras.utils import Sequence, to_categorical\n",
        "from keras import backend as K\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwyBlvquSv3R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the data, add some columns and set up some dictionnaries."
      ]
    },
    {
      "metadata": {
        "id": "mztnLIJ_F-az",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dir_data = '/content/data'\n",
        "n_classes = len(labels)\n",
        "#train = pd.read_csv(os.path.join(dir_data, 'train_curated_clean.csv'))\n",
        "#test = pd.read_csv(os.path.join(dir_data, 'sample_submission.csv'))\n",
        "\n",
        "#train.set_index(\"fname\", inplace=True)\n",
        "#test.set_index(\"fname\", inplace=True)\n",
        "#train[\"label_idx\"] = train.labels.apply(lambda x: label_to_idx[x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ru4064DxXt8N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Configuration class : In particular contains the default settings."
      ]
    },
    {
      "metadata": {
        "id": "2WnwsyPIGQ1V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "\tdef __init__(self, model_name = None, n_classes=n_classes, image_size=(64,64,3), n_folds=1, learning_rate=0.0001, max_epochs=10):\n",
        "\n",
        "\t\tself.n_classes = n_classes\n",
        "\t\tself.image_size = image_size\n",
        "\t\tself.n_folds = n_folds\n",
        "\t\tself.learning_rate = learning_rate\n",
        "\t\tself.max_epochs = max_epochs\n",
        "\t\tself.model_name = model_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6y1g4Yt9Gk7E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Datagenerator class : Inherits from Keras.utils.Sequence to efficiently feed the data.\n",
        "\n",
        "I think that the normalization is done batch by batch, which is not good, as it's done the same way on the test set. "
      ]
    },
    {
      "metadata": {
        "id": "Q_cpm0v2Gf_u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence): # Inherits from Keras.utils.Sequence for multiprocessing\n",
        "\n",
        "    def __init__(self, config, dir_data, list_IDs, labels=None,\n",
        "\t\t\t\t\tbatch_size=16, shuffle=False, augmentation = None, preprocessing_fn=lambda x: x):\n",
        "    \n",
        "        self.config = config\n",
        "        self.dir_data = dir_data\n",
        "        self.list_IDs = list_IDs\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.preprocessing_fn = preprocessing_fn\n",
        "        self.shuffle = shuffle ## DOES IT WORK TO HAVE TRUE ?\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    # returns the number of batches in the Sequence (usually per 1 epoch)\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    # returns a complete batch at the place index\n",
        "    def __getitem__(self,index):\n",
        "        indexes_temp = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes_temp]\n",
        "        return self.__data_generation(list_IDs_temp)\n",
        "\n",
        "    # called at the end of an epoch: reloads IDs\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            print('Got to the end of an epoch. Shuffling the dataset')\n",
        "            np.random.shuffle(self.indexes)\n",
        "    \n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        cur_batch_size = len(list_IDs_temp)\n",
        "        X = np.empty((cur_batch_size, *self.dim)) ### WHAT DIM FOR IMAGES ?\n",
        "        \n",
        "        for i,ID in enumerate(list_IDs_temp):\n",
        "            file_path = os.path.join(self.dir_data, ID)\n",
        "            \n",
        "            # read the image\n",
        "            image = cv2.cvtColor(cv2.imread(file_path), cv2.COLOR_RGB2BGR)\n",
        "            \n",
        "            \n",
        "            # augment, normalize + other preprocesses\n",
        "            image = self.preprocessing_fn(data)  ## REMOVE THAT, SET NONE BY DEFAULT AND DO AN IF self.pre: ...etc\n",
        "            \n",
        "            # save in the big array X\n",
        "            X[i,] = data\n",
        "        \n",
        "        if self.labels is not None:\n",
        "            y = np.empty(cur_batch_size, dtype=int)\n",
        "            for i, ID in enumerate(list_IDs_temp):\n",
        "                y[i] = self.labels[ID]\n",
        "            \n",
        "            return X, to_categorical(y, num_classes=self.config.n_classes)\n",
        "        else:\n",
        "            return X\n",
        "        \n",
        "        \n",
        "        return np.stack([\n",
        "            self.augment(image=x)[\"image\"] for x in batch_x\n",
        "        ], axis=0), np.array(batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v2_qJX-aY8QI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is the dummy 2d model that applies to the audio data after MFCC. Name = dummy2d"
      ]
    },
    {
      "metadata": {
        "id": "q9SoBNfgY7py",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_2d_dummy_model(config):\n",
        "    \n",
        "    nclass = config.n_classes\n",
        "    \n",
        "    inp = Input(shape=(config.dim[0],config.dim[1],1))\n",
        "    x = GlobalMaxPool2D()(inp)\n",
        "    out = Dense(nclass, activation=softmax)(x)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    opt = optimizers.Adam(config.learning_rate)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Euw448aY7cR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And the larger 2D conv model. Name = conv2d"
      ]
    },
    {
      "metadata": {
        "id": "9fN2n-9JY815",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_2d_conv_model(config):\n",
        "    \n",
        "    nclass = config.n_classes\n",
        "    \n",
        "    inp = Input(shape=(config.dim[0],config.dim[1],1))\n",
        "    x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = MaxPool2D()(x)\n",
        "    \n",
        "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = MaxPool2D()(x)\n",
        "    \n",
        "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = MaxPool2D()(x)\n",
        "    \n",
        "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = MaxPool2D()(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(64)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    out = Dense(nclass, activation=softmax)(x)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    opt = optimizers.Adam(config.learning_rate)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6TJhUuuHYXG9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run The Model!"
      ]
    },
    {
      "metadata": {
        "id": "VTmdu86ciq92",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set-up the Paths for the data"
      ]
    },
    {
      "metadata": {
        "id": "LOiwhPZ9irGk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dir_log = os.path.join(dir_data, 'logs')\n",
        "if os.path.isdir(dir_log):\n",
        "        shutil.rmtree(dir_log)\n",
        "\n",
        "dir_pred = os.path.join(dir_data, 'pred')\n",
        "if os.path.isdir(dir_pred):\n",
        "    shutil.rmtree(dir_pred)\n",
        "os.mkdir(dir_pred)\n",
        "\n",
        "dir_train = os.path.join(dir_data, 'curated')\n",
        "dir_test = os.path.join(dir_data, 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G8wNpXt0Y_uH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating the config and other necessary objects"
      ]
    },
    {
      "metadata": {
        "id": "qpdMk0pbYZFG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "config = Config(n_folds=3, learning_rate=0.001, max_epochs=50)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=config.n_folds)\n",
        "\n",
        "augmentation_train = Compose([ \n",
        "    HorizontalFlip(p=0.5),\n",
        "    RandomContrast(limit=0.2, p=0.5),\n",
        "    RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "    RandomBrightness(limit=0.2, p=0.5),\n",
        "    HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20, val_shift_limit=10, p=.9),\n",
        "    # CLAHE(p=1.0, clip_limit=2.0),\n",
        "    ShiftScaleRotate(\n",
        "        shift_limit=0.0625, scale_limit=0.1, \n",
        "        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), \n",
        "    ToFloat(max_value=255)\n",
        "])\n",
        "\n",
        "augmentation_test = Compose([\n",
        "    # CLAHE(p=1.0, clip_limit=2.0),\n",
        "    ToFloat(max_value=255)\n",
        "])\n",
        "\n",
        "\n",
        "#train = train[:size_of_run]\n",
        "#test = test[:size_of_run]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c0f9w0QwbHIB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the Loop"
      ]
    },
    {
      "metadata": {
        "id": "B8R9e3jvZufP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if os.path.isdir(dir_log):\n",
        "        shutil.rmtree(dir_log)\n",
        "if os.path.isdir(dir_pred):\n",
        "    shutil.rmtree(dir_pred)\n",
        "os.mkdir(dir_pred)\n",
        "\n",
        "for i, (train_split, val_split) in enumerate(skf.split(train.index, train.label_idx)):\n",
        "    K.clear_session()\n",
        "    \n",
        "    train_set = train.iloc[train_split]\n",
        "    val_set = train.iloc[val_split]\n",
        "    \n",
        "    callbacks_list= []\n",
        "    \n",
        "    # the checkpoint is causing issues with colab and breaks in the middle of training. A workaround is the restore_best_weights from EarlyStopping\n",
        "    #checkpoint = ModelCheckpoint(os.path.join(dir_log,'best_%d.h5'%i), monitor='val_loss', verbose=1, save_best_only=True)\n",
        "    #callbacks_list.append(checkpoint)  \n",
        "    \n",
        "    early = EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
        "    callbacks_list.append(early)\n",
        "    \n",
        "    tb = TensorBoard(log_dir= os.path.join(dir_log, 'fold_%d'%i), write_graph=True)\n",
        "    callbacks_list.append(tb)    \n",
        "    \n",
        "    print(\"#\"*50)\n",
        "    print(f'\\nFold {i}')\n",
        "    \n",
        "    if config.model_name == 'conv1d':\n",
        "        model = get_1d_conv_model(config)\n",
        "    elif config.model_name == 'conv2d':\n",
        "        model = get_2d_conv_model(config)\n",
        "    elif config.model_name == 'dummy2d':\n",
        "        model = get_2d_dummy_model(config)\n",
        "    else:\n",
        "        model = get_1d_dummy_model(config)\n",
        "    \n",
        "    train_generator = DataGenerator(config, dir_train, train_set.index, labels=train_set.label_idx,\n",
        "\t\t\t\t\tbatch_size=64) #, preprocessing_fn=audio_norm)\n",
        "    val_generator = DataGenerator(config, dir_train, val_set.index, labels=val_set.label_idx,\n",
        "\t\t\t\t\tbatch_size=64) #, preprocessing_fn=audio_norm)\n",
        "\n",
        "    history = model.fit_generator(train_generator, callbacks=callbacks_list, validation_data=val_generator, epochs=config.max_epochs, use_multiprocessing=True, max_queue_size=20)\n",
        "    \n",
        "    model.save(os.path.join(dir_log,'best_%d.h5'%i))\n",
        "    \n",
        "    # training done, now load best model (at what epoch it was best) and predict\n",
        "    model.load_weights(os.path.join(dir_log,'best_%d.h5'%i))\n",
        "    \n",
        "    # save train prediction for error analysis\n",
        "    train_generator = DataGenerator(config, dir_train, train_set.index, labels=train_set.label_idx,\n",
        "\t\t\t\t\tbatch_size=64) #, preprocessing_fn=audio_norm)\n",
        "    \n",
        "    predictions = model.predict_generator(train_generator, use_multiprocessing=True, max_queue_size=20, verbose=1)\n",
        "    np.save(os.path.join(dir_pred, 'train_pred_%d.npy'%i), predictions)\n",
        "    \n",
        "    # save test prediction\n",
        "    test_generator = DataGenerator(config, dir_test, test.index, labels=None,\n",
        "\t\t\t\t\tbatch_size=64) #, preprocessing_fn=audio_norm)\n",
        "    \n",
        "    predictions = model.predict_generator(test_generator, use_multiprocessing=True, max_queue_size=20, verbose=1)\n",
        "    np.save(os.path.join(dir_pred, 'test_pred_%d.npy'%i), predictions)\n",
        "    pred_test_shape = predictions.shape\n",
        "\n",
        "    # Make a submission file\n",
        "    top_3 = np.array(labels)[np.argsort(-predictions, axis=1)[:, :3]]\n",
        "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
        "    test['label'] = predicted_labels\n",
        "    test[['label']].to_csv(os.path.join(dir_pred, 'predictions_%d.csv'%i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hKOd2zzhqAZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BpsqnFZGbNf6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ensemble the Predictions"
      ]
    },
    {
      "metadata": {
        "id": "L-4-EpsVbNoh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_list = []\n",
        "\n",
        "for i in range(config.n_folds):\n",
        "    pred_list.append(np.load(os.path.join(dir_pred, 'test_pred_%d.npy'%i)))\n",
        "    \n",
        "prediction = np.ones_like(pred_list[0])\n",
        "\n",
        "# Taking a geometric mean of the probabilities\n",
        "for pred in pred_list:\n",
        "    prediction = prediction*pred\n",
        "prediction_gm = prediction**(1./len(pred_list))\n",
        "\n",
        "# Make a submission file\n",
        "top_3 = np.array(labels)[np.argsort(-prediction_gm, axis=1)[:, :3]]\n",
        "\n",
        "predicted_labels = [' '.join(list(x)) for x in top_3]\n",
        "\n",
        "submission = pd.read_csv(os.path.join(dir_data, 'sample_submission.csv'))\n",
        "\n",
        "submission['label'] = predicted_labels\n",
        "submission[['fname', 'label']].to_csv(os.path.join(dir_data, 'submission.csv'), index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3vHWTByy9oG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now save the logs, pr"
      ]
    },
    {
      "metadata": {
        "id": "R7FZv0c4mbxV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "datetime_now = str(datetime.now()).replace('-','').replace(':','').replace(' ','')\n",
        "datetime_now = datetime_now[:datetime_now.find('.')]\n",
        "datetime_now = datetime_now[4:8] + '_' + datetime_now[-6:-2]\n",
        "\n",
        "# save the logs\n",
        "zipname = datetime_now + '_log.zip'\n",
        "!zip -r /content/\"$zipname\" /content/logs\n",
        "files.download(os.path.join(dir_data, zipname))\n",
        "\n",
        "# save the preds\n",
        "zipname = datetime_now + '_pred.zip'\n",
        "!zip -r /content/\"$zipname\" /content/pred\n",
        "files.download(os.path.join(dir_data, zipname))\n",
        "\n",
        "# save the submission\n",
        "zipname = datetime_now + '_submission.zip'\n",
        "!zip -r /content/\"$zipname\" /content/submission.csv\n",
        "files.download(os.path.join(dir_data, zipname))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcAlnIYuvQT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}